torch
pytest
flash-linear-attention
transformers